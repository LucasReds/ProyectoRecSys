{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7b62f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import transformers\n",
    "import json\n",
    "import torch\n",
    "#import isodate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0688d550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522517, 28) (1401982, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>Name</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>CookTime</th>\n",
       "      <th>PrepTime</th>\n",
       "      <th>TotalTime</th>\n",
       "      <th>DatePublished</th>\n",
       "      <th>Description</th>\n",
       "      <th>Images</th>\n",
       "      <th>...</th>\n",
       "      <th>SaturatedFatContent</th>\n",
       "      <th>CholesterolContent</th>\n",
       "      <th>SodiumContent</th>\n",
       "      <th>CarbohydrateContent</th>\n",
       "      <th>FiberContent</th>\n",
       "      <th>SugarContent</th>\n",
       "      <th>ProteinContent</th>\n",
       "      <th>RecipeServings</th>\n",
       "      <th>RecipeYield</th>\n",
       "      <th>RecipeInstructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>Low-Fat Berry Blue Frozen Dessert</td>\n",
       "      <td>1533</td>\n",
       "      <td>Dancer</td>\n",
       "      <td>PT24H</td>\n",
       "      <td>PT45M</td>\n",
       "      <td>PT24H45M</td>\n",
       "      <td>1999-08-09T21:46:00Z</td>\n",
       "      <td>Make and share this Low-Fat Berry Blue Frozen ...</td>\n",
       "      <td>c(\"https://img.sndimg.com/food/image/upload/w_...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29.8</td>\n",
       "      <td>37.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>30.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c(\"Toss 2 cups berries with sugar.\", \"Let stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>Biryani</td>\n",
       "      <td>1567</td>\n",
       "      <td>elly9812</td>\n",
       "      <td>PT25M</td>\n",
       "      <td>PT4H</td>\n",
       "      <td>PT4H25M</td>\n",
       "      <td>1999-08-29T13:12:00Z</td>\n",
       "      <td>Make and share this Biryani recipe from Food.com.</td>\n",
       "      <td>c(\"https://img.sndimg.com/food/image/upload/w_...</td>\n",
       "      <td>...</td>\n",
       "      <td>16.6</td>\n",
       "      <td>372.8</td>\n",
       "      <td>368.4</td>\n",
       "      <td>84.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>63.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c(\"Soak saffron in warm milk for 5 minutes and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>Best Lemonade</td>\n",
       "      <td>1566</td>\n",
       "      <td>Stephen Little</td>\n",
       "      <td>PT5M</td>\n",
       "      <td>PT30M</td>\n",
       "      <td>PT35M</td>\n",
       "      <td>1999-09-05T19:52:00Z</td>\n",
       "      <td>This is from one of my  first Good House Keepi...</td>\n",
       "      <td>c(\"https://img.sndimg.com/food/image/upload/w_...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>81.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>77.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c(\"Into a 1 quart Jar with tight fitting lid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>Carina's Tofu-Vegetable Kebabs</td>\n",
       "      <td>1586</td>\n",
       "      <td>Cyclopz</td>\n",
       "      <td>PT20M</td>\n",
       "      <td>PT24H</td>\n",
       "      <td>PT24H20M</td>\n",
       "      <td>1999-09-03T14:54:00Z</td>\n",
       "      <td>This dish is best prepared a day in advance to...</td>\n",
       "      <td>c(\"https://img.sndimg.com/food/image/upload/w_...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1558.6</td>\n",
       "      <td>64.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>32.1</td>\n",
       "      <td>29.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4 kebabs</td>\n",
       "      <td>c(\"Drain the tofu, carefully squeezing out exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Cabbage Soup</td>\n",
       "      <td>1538</td>\n",
       "      <td>Duckie067</td>\n",
       "      <td>PT30M</td>\n",
       "      <td>PT20M</td>\n",
       "      <td>PT50M</td>\n",
       "      <td>1999-09-19T06:19:00Z</td>\n",
       "      <td>Make and share this Cabbage Soup recipe from F...</td>\n",
       "      <td>\"https://img.sndimg.com/food/image/upload/w_55...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>959.3</td>\n",
       "      <td>25.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>17.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c(\"Mix everything together and bring to a boil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecipeId                               Name  AuthorId      AuthorName  \\\n",
       "0        38  Low-Fat Berry Blue Frozen Dessert      1533          Dancer   \n",
       "1        39                            Biryani      1567        elly9812   \n",
       "2        40                      Best Lemonade      1566  Stephen Little   \n",
       "3        41     Carina's Tofu-Vegetable Kebabs      1586         Cyclopz   \n",
       "4        42                       Cabbage Soup      1538       Duckie067   \n",
       "\n",
       "  CookTime PrepTime TotalTime         DatePublished  \\\n",
       "0    PT24H    PT45M  PT24H45M  1999-08-09T21:46:00Z   \n",
       "1    PT25M     PT4H   PT4H25M  1999-08-29T13:12:00Z   \n",
       "2     PT5M    PT30M     PT35M  1999-09-05T19:52:00Z   \n",
       "3    PT20M    PT24H  PT24H20M  1999-09-03T14:54:00Z   \n",
       "4    PT30M    PT20M     PT50M  1999-09-19T06:19:00Z   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Make and share this Low-Fat Berry Blue Frozen ...   \n",
       "1  Make and share this Biryani recipe from Food.com.   \n",
       "2  This is from one of my  first Good House Keepi...   \n",
       "3  This dish is best prepared a day in advance to...   \n",
       "4  Make and share this Cabbage Soup recipe from F...   \n",
       "\n",
       "                                              Images  ... SaturatedFatContent  \\\n",
       "0  c(\"https://img.sndimg.com/food/image/upload/w_...  ...                 1.3   \n",
       "1  c(\"https://img.sndimg.com/food/image/upload/w_...  ...                16.6   \n",
       "2  c(\"https://img.sndimg.com/food/image/upload/w_...  ...                 0.0   \n",
       "3  c(\"https://img.sndimg.com/food/image/upload/w_...  ...                 3.8   \n",
       "4  \"https://img.sndimg.com/food/image/upload/w_55...  ...                 0.1   \n",
       "\n",
       "  CholesterolContent SodiumContent CarbohydrateContent  FiberContent  \\\n",
       "0                8.0          29.8                37.1           3.6   \n",
       "1              372.8         368.4                84.4           9.0   \n",
       "2                0.0           1.8                81.5           0.4   \n",
       "3                0.0        1558.6                64.2          17.3   \n",
       "4                0.0         959.3                25.1           4.8   \n",
       "\n",
       "   SugarContent  ProteinContent  RecipeServings  RecipeYield  \\\n",
       "0          30.2             3.2             4.0          NaN   \n",
       "1          20.4            63.4             6.0          NaN   \n",
       "2          77.2             0.3             4.0          NaN   \n",
       "3          32.1            29.3             2.0     4 kebabs   \n",
       "4          17.7             4.3             4.0          NaN   \n",
       "\n",
       "                                  RecipeInstructions  \n",
       "0  c(\"Toss 2 cups berries with sugar.\", \"Let stan...  \n",
       "1  c(\"Soak saffron in warm milk for 5 minutes and...  \n",
       "2  c(\"Into a 1 quart Jar with tight fitting lid, ...  \n",
       "3  c(\"Drain the tofu, carefully squeezing out exc...  \n",
       "4  c(\"Mix everything together and bring to a boil...  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSVs\n",
    "recipes = pd.read_csv(\"recipes.csv\", dtype={'CookTime': str, 'PrepTime': str, 'TotalTime': str, 'RecipeId': int})\n",
    "reviews = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "print(recipes.shape, reviews.shape)\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e1d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522517, 28) (1401982, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified  \n",
       "0  2000-01-25T21:44:00Z  \n",
       "1  2001-10-17T16:49:59Z  \n",
       "2  2000-02-25T09:00:00Z  \n",
       "3  2000-03-13T21:15:00Z  \n",
       "4  2000-03-28T12:51:00Z  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(recipes.shape, reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b2bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate recipes/reviews\n",
    "recipes = recipes.drop_duplicates(subset=\"RecipeId\")\n",
    "reviews = reviews.drop_duplicates(subset=\"ReviewId\")\n",
    "\n",
    "nutritional_cols = [\n",
    "    \"Calories\",\"FatContent\",\"SaturatedFatContent\",\"CholesterolContent\",\n",
    "    \"SodiumContent\",\"CarbohydrateContent\",\"FiberContent\",\"SugarContent\",\"ProteinContent\"\n",
    "]\n",
    "for col in nutritional_cols:\n",
    "    recipes[col] = pd.to_numeric(recipes[col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0a1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10): # Obtenida de https://surprise.readthedocs.io/en/stable/FAQ.html\n",
    "    \"\"\"Devuelve el top-N para cada usuario a partir de un conjunto de predicciones.\"\"\"\n",
    "\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Ordenar predicciones por score\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48042173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(recs, relevant, k=10):\n",
    "    recs_k = recs[:k]\n",
    "    hits = len(set(recs_k) & set(relevant))\n",
    "    precision = hits / k if k else 0\n",
    "    recall = hits / len(relevant) if relevant else 0\n",
    "    return precision, recall\n",
    "\n",
    "def ndcg_at_k(recs, relevant, k=10):\n",
    "    recs_k = recs[:k]\n",
    "    dcg = sum(1 / math.log2(i+2) for i, iid in enumerate(recs_k) if iid in relevant)\n",
    "    idcg = sum(1 / math.log2(i+2) for i in range(min(len(relevant), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8f222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RecipeId NutriScore\n",
      "0         38          C\n",
      "1         39          D\n",
      "2         40          D\n",
      "3         41          D\n",
      "4         42          C\n",
      "5         43          D\n",
      "6         44          D\n",
      "7         45          C\n",
      "8         46          B\n",
      "9         47          B\n",
      "10        48          E\n",
      "11        49          D\n",
      "12        50          B\n",
      "13        51          B\n",
      "14        52          B\n",
      "15        53          C\n",
      "16        54          D\n",
      "17        55          C\n",
      "18        56          E\n",
      "19        57          A\n"
     ]
    }
   ],
   "source": [
    "def nutri_score(row):\n",
    "    # Negative components\n",
    "    neg = 0\n",
    "    if not pd.isna(row[\"Calories\"]): neg += min(10, row[\"Calories\"] / 335)  # ~335 kJ = 80 kcal per point\n",
    "    if not pd.isna(row[\"SugarContent\"]): neg += min(10, row[\"SugarContent\"] / 4.5)\n",
    "    if not pd.isna(row[\"SaturatedFatContent\"]): neg += min(10, row[\"SaturatedFatContent\"] / 1)\n",
    "    if not pd.isna(row[\"SodiumContent\"]): neg += min(10, row[\"SodiumContent\"] / 90)\n",
    "\n",
    "    # Positive components\n",
    "    pos = 0\n",
    "    if not pd.isna(row[\"FiberContent\"]): pos += min(5, row[\"FiberContent\"] / 0.9)\n",
    "    if not pd.isna(row[\"ProteinContent\"]): pos += min(5, row[\"ProteinContent\"] / 1.6)\n",
    "\n",
    "    score = neg - pos\n",
    "    if score <= -1: return \"A\"\n",
    "    elif score <= 2: return \"B\"\n",
    "    elif score <= 10: return \"C\"\n",
    "    elif score <= 18: return \"D\"\n",
    "    else: return \"E\"\n",
    "\n",
    "recipes[\"NutriScore\"] = recipes.apply(nutri_score, axis=1)\n",
    "print(recipes[[\"RecipeId\", \"NutriScore\"]].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d3af817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Devuelve las N-mejores recomendaciones para cada usuario de un set de predicción.\n",
    "\n",
    "    Args:\n",
    "        predictions(lista de objetos Prediction): La lista de predicción obtenida del método test.\n",
    "        n(int): El número de recomendaciónes por usuario\n",
    "\n",
    "    Returns:\n",
    "    Un diccionario donde las llaves son ids de usuario y los valores son listas de tuplas:\n",
    "        [(item id, rating estimation), ...] de tamaño n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9a9c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(top_n, test_user_items, k=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for uid, recs in top_n.items():\n",
    "        if uid not in test_user_items:\n",
    "            continue\n",
    "        relevant = set(test_user_items[uid])\n",
    "        recommended = set([iid for iid, _ in recs[:k]])\n",
    "        precisions.append(len(recommended & relevant) / k)\n",
    "        recalls.append(len(recommended & relevant) / len(relevant))\n",
    "    return np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "def ndcg_at_k(top_n, test_user_items, k=10):\n",
    "    ndcgs = []\n",
    "    for uid, recs in top_n.items():\n",
    "        if uid not in test_user_items:\n",
    "            continue\n",
    "        relevant = set(test_user_items[uid])\n",
    "        dcg = 0\n",
    "        for i, (iid, _) in enumerate(recs[:k]):\n",
    "            if iid in relevant:\n",
    "                dcg += 1 / math.log2(i + 2) \n",
    "        idcg = sum(1 / math.log2(i + 2) for i in range(min(k, len(relevant))))\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4dc4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucas\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files: 100%|██████████| 2/2 [01:08<00:00, 34.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",  # cache folder for CPU layers\n",
    "    max_memory={\n",
    "        0: \"6GiB\",    # GPU\n",
    "        \"cpu\": \"16GiB\"\n",
    "    }\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d17f7ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.dense\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.dense\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.dense\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.dense\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.dense\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.dense\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.dense\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.dense\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.dense\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.dense\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.dense\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.dense\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.dense\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.dense\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.dense\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.dense\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.dense\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.dense\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.dense\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.dense\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.dense\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.dense\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.dense\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.dense\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.dense\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.dense\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.dense\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.dense\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.dense\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.dense\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.dense\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.dense\n"
     ]
    }
   ],
   "source": [
    "for name, _ in model.named_modules():\n",
    "    if \"attn\" in name or \"proj\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ebca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,242,880 || all params: 2,784,926,720 || trainable%: 0.1883\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376fe6dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m user_histories = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m reviews.iterrows():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     rid = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecipeId\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rid \u001b[38;5;129;01min\u001b[39;00m recipe_names:\n\u001b[32m      9\u001b[39m         user_histories[row[\u001b[33m\"\u001b[39m\u001b[33mAuthorId\u001b[39m\u001b[33m\"\u001b[39m]].append((recipe_names[rid], row[\u001b[33m\"\u001b[39m\u001b[33mRating\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:1239\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m   1237\u001b[39m loc = \u001b[38;5;28mself\u001b[39m.index.get_loc(label)\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, MultiIndex):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Build a dictionary for O(1) lookup of recipe names\n",
    "recipe_names = dict(zip(recipes[\"RecipeId\"], recipes[\"Name\"]))\n",
    "\n",
    "# Build user histories fast\n",
    "user_histories = defaultdict(list)\n",
    "for _, row in reviews.iterrows():\n",
    "    rid = row[\"RecipeId\"]\n",
    "    if rid in recipe_names:\n",
    "        user_histories[row[\"AuthorId\"]].append((recipe_names[rid], row[\"Rating\"]))\n",
    "\n",
    "# Convert defaultdict to regular dict before saving\n",
    "user_histories = dict(user_histories)\n",
    "\n",
    "# Save to disk (JSON format)\n",
    "with open(\"user_histories.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_histories, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11ef27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user histories if already saved\n",
    "with open(\"user_histories.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    user_histories = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c4e3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "\n",
    "samples = []\n",
    "for _, row in reviews.sample(10000, random_state=42).iterrows():  # limit for now\n",
    "    uid, rid, rating = row[\"AuthorId\"], row[\"RecipeId\"], row[\"Rating\"]\n",
    "    recipe = recipes.loc[recipes[\"RecipeId\"] == rid]\n",
    "    if recipe.empty:\n",
    "        continue\n",
    "\n",
    "    name = recipe.iloc[0][\"Name\"]\n",
    "    nutri = recipe.iloc[0][\"NutriScore\"]\n",
    "    history = user_histories[uid][-5:] if uid in user_histories else []\n",
    "    history_text = \"\\n\".join([f\"- {rname} ({rscore}/5)\" for rname, rscore in history]) or \"No previous ratings.\"\n",
    "\n",
    "    prompt = (\n",
    "        \"A user has rated several recipes before. Based on their preferences \"\n",
    "        \"and the healthiness of the recipe (NutriScore A–E), predict their rating (1–5).\\n\\n\"\n",
    "        f\"User history:\\n{history_text}\\n\\n\"\n",
    "        f\"Target recipe: {name} (NutriScore {nutri})\\n\"\n",
    "        \"Answer only with a single number between 1 and 5 inclusive.\"\n",
    "    )\n",
    "\n",
    "    response = str(int(round(rating)))\n",
    "    samples.append({\"prompt\": prompt, \"response\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f1b0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in samples[:4000]:\n",
    "        f.write(json.dumps(s) + \"\\n\")\n",
    "\n",
    "with open(\"val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in samples[4000:]:\n",
    "        f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0f81740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4000 examples [00:00, 128164.27 examples/s]\n",
      "Generating validation split: 5999 examples [00:00, 584270.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"validation\": \"val.jsonl\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a85b0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:01<00:00, 2537.38 examples/s]\n",
      "Map: 100%|██████████| 5999/5999 [00:02<00:00, 2598.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"})\n",
    "\n",
    "def format_dataset(example):\n",
    "    text = f\"{example['prompt']}\\n\\n{example['response']}\"\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(format_dataset)\n",
    "val_dataset = dataset[\"validation\"].map(format_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b679577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:40:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.055683947265148165, metrics={'train_runtime': 6057.3619, 'train_samples_per_second': 0.66, 'train_steps_per_second': 0.165, 'total_flos': 3.261056679936e+16, 'train_loss': 0.055683947265148165, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi2-finetuned\",\n",
    "    per_device_train_batch_size=1,     \n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    eval_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a0a9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./phi2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./phi2-finetuned\")\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a61afe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./phi2-merged\\\\tokenizer_config.json',\n",
       " './phi2-merged\\\\special_tokens_map.json',\n",
       " './phi2-merged\\\\vocab.json',\n",
       " './phi2-merged\\\\merges.txt',\n",
       " './phi2-merged\\\\added_tokens.json',\n",
       " './phi2-merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model_name = \"microsoft/phi-2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(base_model, \"./phi2-finetuned\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"./phi2-merged\")\n",
    "tokenizer.save_pretrained(\"./phi2-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa7236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class LLMPredictor:\n",
    "    def __init__(self, recipes, user_histories, batch_size=10, model_path=\"./phi2-merged\"):\n",
    "        self.recipes = recipes\n",
    "        self.user_histories = user_histories\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"Model loaded and ready.\")\n",
    "\n",
    "        # Precompute token IDs for digits 1–5\n",
    "        self.rating_token_ids = {\n",
    "            i: self.tokenizer.encode(str(i), add_special_tokens=False)[0]\n",
    "            for i in range(1, 6)\n",
    "        }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # FAST single prediction (NO autoregressive generation)\n",
    "    # ------------------------------------------------------------------\n",
    "    def _predict_single(self, prompt: str) -> int:\n",
    "        \"\"\"Run a fast forward pass and decode ONLY the final token as rating.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**inputs)\n",
    "            logits = output.logits[:, -1, :]  # only the last token\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Choose the rating token with the highest probability\n",
    "        best_rating = max(\n",
    "            self.rating_token_ids.keys(),\n",
    "            key=lambda r: probs[0, self.rating_token_ids[r]].item()\n",
    "        )\n",
    "\n",
    "        return int(best_rating)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Batch prediction\n",
    "    # ------------------------------------------------------------------\n",
    "    def predict_batch(self, pairs):\n",
    "        \"\"\"\n",
    "        pairs: list of (user_id, recipe_id, true_rating)\n",
    "        Returns: list of predicted ratings (ints)\n",
    "        \"\"\"\n",
    "\n",
    "        preds = []\n",
    "        for (uid, iid, true_r) in pairs:\n",
    "\n",
    "            # --- Get recipe ---\n",
    "            row = self.recipes.loc[self.recipes[\"RecipeId\"] == iid]\n",
    "            if row.empty:\n",
    "                preds.append(3)  # fallback\n",
    "                continue\n",
    "\n",
    "            recipe = row.iloc[0]\n",
    "\n",
    "            # --- User history ---\n",
    "            history_list = self.user_histories.get(uid, [])\n",
    "            if history_list:\n",
    "                history_text = \"\\n\".join(\n",
    "                    f\"- {name}: rated {rating}\"\n",
    "                    for name, rating in history_list[-5:]\n",
    "                )\n",
    "            else:\n",
    "                history_text = \"No past ratings available.\"\n",
    "\n",
    "            # --- Build prompt ---\n",
    "            # Single example, optimized for classification\n",
    "            prompt = (\n",
    "                \"You are a model that predicts how much a user will rate a recipe.\\n\"\n",
    "                \"Respond ONLY with a single number from 1 to 5 (no words).\\n\"\n",
    "                \"Higher numbers mean the user will like the recipe more.\\n\"\n",
    "                \"You should also favor recipes with a healthier NutriScore.\\n\\n\"\n",
    "                f\"User ID: {uid}\\n\\n\"\n",
    "                f\"User History:\\n{history_text}\\n\\n\"\n",
    "                f\"Recipe:\\n\"\n",
    "                f\"- Name: {recipe['Name']}\\n\"\n",
    "                f\"- Health Score (NutriScore): {recipe['NutriScore']}\\n\\n\"\n",
    "                \"Final answer (1–5):\"\n",
    "            )\n",
    "\n",
    "            # --- Predict fast ---\n",
    "            try:\n",
    "                pred = self._predict_single(prompt)\n",
    "            except Exception:\n",
    "                pred = 3  # fallback\n",
    "\n",
    "            preds.append(pred)\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee29238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230674/230674 [05:10<00:00, 742.28it/s] \n"
     ]
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise import Reader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = reviews[[\"AuthorId\", \"RecipeId\", \"Rating\"]]\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "recipes[\"RecipeId\"] = recipes[\"RecipeId\"].astype(int)\n",
    "train_df[\"RecipeId\"] = train_df[\"RecipeId\"].astype(int)\n",
    "test_df[\"RecipeId\"] = test_df[\"RecipeId\"].astype(int)\n",
    "\n",
    "# Build user histories\n",
    "user_histories = {}\n",
    "grouped = train_df.groupby(\"AuthorId\")\n",
    "for uid, df in tqdm(grouped):\n",
    "    # Keep recipe name + rating\n",
    "    user_histories[uid] = [\n",
    "        (recipes.loc[recipes[\"RecipeId\"] == rid, \"Name\"].values[0], int(r))\n",
    "        for rid, r in zip(df[\"RecipeId\"], df[\"Rating\"])\n",
    "        if rid in recipes[\"RecipeId\"].values\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2ef517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./phi2-merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [40:51<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "recipes[\"RecipeId\"] = recipes[\"RecipeId\"].astype(int)\n",
    "train_df[\"RecipeId\"] = train_df[\"RecipeId\"].astype(int)\n",
    "test_df[\"RecipeId\"] = test_df[\"RecipeId\"].astype(int)\n",
    "\n",
    "# Take a smaller sample of the test set\n",
    "sample_size = 10000  # adjust depending on patience/budget\n",
    "sampled_test = test_df.sample(n=sample_size, random_state=4).reset_index(drop=True)\n",
    "\n",
    "# Initialize model\n",
    "llm_model = LLMPredictor(recipes, user_histories, batch_size=10, model_path=\"./phi2-merged\")\n",
    "\n",
    "predictions = []\n",
    "rows = sampled_test.to_dict(orient=\"records\")\n",
    "\n",
    "for i in tqdm(range(0, len(rows), llm_model.batch_size)):\n",
    "    batch = rows[i:i + llm_model.batch_size]\n",
    "    pairs = [(r[\"AuthorId\"], r[\"RecipeId\"], r[\"Rating\"]) for r in batch]\n",
    "    ests = llm_model.predict_batch(pairs, )\n",
    "    for (uid, iid, true_r), est in zip(pairs, ests):\n",
    "        #print(\"Estimate for user\", uid, \"and recipe\", iid, \"is\", est, \"true rating is\", true_r)\n",
    "        predictions.append((uid, iid, true_r, est, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93d5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from microsoft/phi-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.81it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 237.99it/s]\n"
     ]
    }
   ],
   "source": [
    "recipes[\"RecipeId\"] = recipes[\"RecipeId\"].astype(int)\n",
    "train_df[\"RecipeId\"] = train_df[\"RecipeId\"].astype(int)\n",
    "test_df[\"RecipeId\"] = test_df[\"RecipeId\"].astype(int)\n",
    "\n",
    "# Take a smaller sample of the test set\n",
    "sample_size = 10000  # adjust depending on patience/budget\n",
    "sampled_test = test_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# Initialize model\n",
    "llm_model = LLMPredictor(recipes, user_histories, batch_size=10, model_path=model_name)\n",
    "\n",
    "predictions_base = []\n",
    "rows = sampled_test.to_dict(orient=\"records\")\n",
    "\n",
    "for i in tqdm(range(0, len(rows), llm_model.batch_size)):\n",
    "    batch = rows[i:i + llm_model.batch_size]\n",
    "    pairs = [(r[\"AuthorId\"], r[\"RecipeId\"], r[\"Rating\"]) for r in batch]\n",
    "    ests = llm_model.predict_batch(pairs, )\n",
    "    for (uid, iid, true_r), est in zip(pairs, ests):\n",
    "        #print(\"Estimate for user\", uid, \"and recipe\", iid, \"is\", est, \"true rating is\", true_r)\n",
    "        predictions_base.append((uid, iid, true_r, est, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb4f97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101174784\n",
      "1100048384\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./tinyllama-finetuned2\")\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "base = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(sum(p.numel() for p in base.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12aa154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged params: 1100048384\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "m = AutoModelForCausalLM.from_pretrained(\"./tinyllama-merged2\")\n",
    "print(\"Merged params:\", sum(p.numel() for p in m.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81990c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ./tinyllama-finetuned2 ===\n",
      "README.md 0.01 MB\n",
      "adapter_config.json 0.0 MB\n",
      "adapter_model.safetensors 4.52 MB\n",
      "chat_template.jinja 0.0 MB\n",
      "special_tokens_map.json 0.0 MB\n",
      "tokenizer.json 3.62 MB\n",
      "tokenizer_config.json 0.0 MB\n",
      "=== ./tinyllama-merged2 ===\n",
      "chat_template.jinja 0.0 MB\n",
      "config.json 0.0 MB\n",
      "generation_config.json 0.0 MB\n",
      "model.safetensors 4400.22 MB\n",
      "special_tokens_map.json 0.0 MB\n",
      "tokenizer.json 3.62 MB\n",
      "tokenizer_config.json 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# check files/sizes in adapter folder\n",
    "import os\n",
    "for p in [\"./tinyllama-finetuned2\", \"./tinyllama-merged2\"]:\n",
    "    print(\"===\", p, \"===\")\n",
    "    if not os.path.isdir(p):\n",
    "        print(\"MISSING\", p); continue\n",
    "    for f in sorted(os.listdir(p)):\n",
    "        fp = os.path.join(p, f)\n",
    "        if os.path.isfile(fp):\n",
    "            print(f, round(os.path.getsize(fp)/1e6,2), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5c7abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.582\n",
      "RMSE: 1.388\n",
      "Base model:\n",
      "MAE: 1.808\n",
      "RMSE: 1.899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.8077, 1.8989734068701436)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def compute_mae(predictions):\n",
    "    \"\"\"Compute Mean Absolute Error.\"\"\"\n",
    "    true = [t for _, _, t, _, _ in predictions]\n",
    "    pred = [p for _, _, _, p, _ in predictions]\n",
    "    return mean_absolute_error(true, pred)\n",
    "\n",
    "def compute_rmse(predictions):\n",
    "    \"\"\"Compute Root Mean Squared Error.\"\"\"\n",
    "    true = [t for _, _, t, _, _ in predictions]\n",
    "    pred = [p for _, _, _, p, _ in predictions]\n",
    "    return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    mae = compute_mae(predictions)\n",
    "    rmse = compute_rmse(predictions)\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "mae, rmse = compute_metrics(predictions)\n",
    "print(\"Base model:\")\n",
    "compute_metrics(predictions_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d043e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n(predictions, n=10, min_rating=1, max_rating=5):\n",
    "    \"\"\"\n",
    "    Devuelve las N-mejores recomendaciones para cada usuario de un set de predicciones.\n",
    "\n",
    "    Args:\n",
    "        predictions (list[tuple]): Lista de tuplas (uid, iid, true_r, est, _)\n",
    "        n (int): Número de recomendaciones por usuario\n",
    "        min_rating (float): Valor mínimo de rating permitido\n",
    "        max_rating (float): Valor máximo de rating permitido\n",
    "\n",
    "    Returns:\n",
    "        dict: {uid: [(iid, est), ...]} con las N mejores recomendaciones por usuario\n",
    "    \"\"\"\n",
    "    top_n = defaultdict(list)\n",
    "\n",
    "    # Asocia todas las predicciones al usuario correspondiente\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        # Ignorar predicciones inválidas o nulas\n",
    "        if est is None:\n",
    "            continue\n",
    "        # Asegurar que el rating esté en rango válido\n",
    "        est = float(est)\n",
    "        est = min(max(est, min_rating), max_rating)\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Ordenar y recortar las mejores N recomendaciones\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return dict(top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64e7b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_items = defaultdict(list)\n",
    "for uid, iid, r in zip(test_df[\"AuthorId\"], test_df[\"RecipeId\"], test_df[\"Rating\"]):\n",
    "    if r >= 4:  # only consider ratings 4 or 5 as relevant\n",
    "        test_user_items[uid].append(iid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fee0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.144\n",
      "Recall@10:    0.417\n",
      "NDCG@10:      0.559\n"
     ]
    }
   ],
   "source": [
    "top_n = get_top_n(predictions, n=10)\n",
    "precision, recall = precision_recall_at_k(top_n, test_user_items, k=10)\n",
    "ndcg = ndcg_at_k(top_n, test_user_items, k=10)\n",
    "\n",
    "print(f\"Precision@10: {precision:.3f}\")\n",
    "print(f\"Recall@10:    {recall:.3f}\")\n",
    "print(f\"NDCG@10:      {ndcg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69fc1cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.143\n",
      "Recall@10:    0.422\n",
      "NDCG@10:      0.561\n"
     ]
    }
   ],
   "source": [
    "top_n = get_top_n(predictions_base, n=10)\n",
    "precision, recall = precision_recall_at_k(top_n, test_user_items, k=10)\n",
    "ndcg = ndcg_at_k(top_n, test_user_items, k=10)\n",
    "\n",
    "print(f\"Precision@10: {precision:.3f}\")\n",
    "print(f\"Recall@10:    {recall:.3f}\")\n",
    "print(f\"NDCG@10:      {ndcg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75b1e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanHealth@10: 2.8044\n"
     ]
    }
   ],
   "source": [
    "# Map NutriScore letters to numeric values if needed, e.g. A=5, B=4, ..., E=1\n",
    "nutri_map = {\"A\": 5, \"B\": 4, \"C\": 3, \"D\": 2, \"E\": 1}\n",
    "\n",
    "def mean_health_at_k(top_n, recipes_df, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean health (NutriScore) for the top-k recommendations per user.\n",
    "    \n",
    "    Args:\n",
    "        top_n: dict of user_id -> [(recipe_id, predicted_rating), ...]\n",
    "        recipes_df: dataframe with columns ['RecipeId', 'NutriScore']\n",
    "        k: top-k items to consider per user\n",
    "        \n",
    "    Returns:\n",
    "        mean_health: float, mean NutriScore (numeric) of top-k items\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    for uid, recs in top_n.items():\n",
    "        for iid, _ in recs[:k]:\n",
    "            row = recipes_df.loc[recipes_df[\"RecipeId\"] == iid]\n",
    "            if row.empty:\n",
    "                continue\n",
    "            score_letter = row.iloc[0][\"NutriScore\"]\n",
    "            score = nutri_map.get(score_letter, np.nan)\n",
    "            if not np.isnan(score):\n",
    "                all_scores.append(score)\n",
    "    \n",
    "    return np.mean(all_scores) if all_scores else np.nan\n",
    "\n",
    "# Example usage:\n",
    "# top_n = get_top_n(predictions, n=10)\n",
    "mean_health = mean_health_at_k(top_n, recipes, k=5)\n",
    "print(f\"MeanHealth@10: {mean_health:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
